#+BEGIN_HTML
---
date: 2015-03-14 21:28:16
template: tech.jade
title: Netty 1M 链接压测
category: Java
chage_frequency: monthly
tag: Java, Netty
---
#+END_HTML
#+OPTIONS: toc:nil
#+TOC: headlines 2

工作需要对服务进行性能评估。服务采用的是Netty通讯。本文仅就Netty空载下的压力测试进行纪录。

*** 压测环境
| 目标     | Cpu | 内存(G) | 程序   | 堆       |
|----------+-----+---------+--------+----------|
| 被压机器 |   4 |       8 | Netty  | 自由增长 |
| 施压机器 |   8 |      32 | Erlang | -        |
|----------+-----+---------+--------+----------|

*** 通讯协议
基于2进制的TCP上的报文
*** 客户端

**** 客户端程序
客户端采用Erlang 语言来编写，实现简单的客户端逻辑，同时利用Erlang的高并发特性，可以创建大量的进程，每个进程模拟一个客户端，内存占用很少，采用Java的话内存使用率不如它高效。

Erlang语言编写网络程序非常简单，这也为及时调整客户端加入一些策略提供便捷。

在压测中每个Erlang进程启动了5w个进程，模拟5w

启动Erlang 虚拟机的时候，记得加上 : =+K true= 开启利用系统的poll功能，linux下面是epoll，会更高效。

**** 客户端系统
客户端由于要发起大量的链接，所以需要调整很多网络和OS的参数：

- nofile :: 修改 /etc/security/limits.conf 改大单会话的对打开文件数的limit。
	    修改过后进行重新登入，sshd 会加载pam的limits配置。
- /proc/sys/fs/file-max :: 全局文件数限制，此参数限制系统能够打开的最大文件数，位于 =/proc/sys/fs/file-max= 修改为尽可能大，如100w。
- /proc/sys/net/ipv4/ip_local_port_range :: 此参数限制一个ip上面能够打开的端口数，默认的不足使用，需要改成大一些的，比如我这里改成大于5w : =1024 65535= ，注意这里设置为65535 之上是没有用的。
- /proc/sys/net/netfilter/nf_conntrack_max :: 链接跟踪的最大容量，默认的会报此表已满，调到100w。

**** 客户端端口限制
由于tcp/udp 设计之处，port编码是16位，即用2个字节来表示端口，这也端口的最大值为： =2^16 - 1 = 65535= 这也就是说对于一个Ip我们只有这么多个对外链接可以建立。

为了在单台服务器上面启动更多的client，建立更多的对外链接，就需要增加ip，在链接建立的时候，指定本地ip为不同的ip，因为每个ip有65535个端口可用，于是增加了可以建立的连接数：
=count = ipNum * 65535=

本次设置了10个网卡，这也就可以建立50w的链接。

遗憾的是在2.18的内核下面无法通过，还是总量上只能启动65535个端口。

换用新内核后，完美解决。

**** 客户端表现

10w链接 -> 2.4g内存，load 7， cpu idle : 41%
106.18k的时候，机器挂掉了。

再次启动，观察各个cpu的使用，发现si项有一个cpu特别高，然后想到了之前听说过的网卡软中断不均衡的问题，

首先找到网卡使用的中断号：
#+BEGIN_SRC shell
cat /proc/interrupts | grep "eth"
#+END_SRC
最前面一列就是中断号，而后面的各个列是每个cpu一个列，代表了此cpu上此中断发生的次数。

注意我们说的中断是软中断，指的是硬中断的后半部分，中断发生在网卡数据包到达的时候，硬件中断发出来通知cpu数据到来。

cpu将处理中断的上半部分，然后将其他可以延迟的任务放入软中断队列。

通过设置 =/proc/irq/<19>/sm_affinity= 来将中断的处理程序绑定到cpu。这里可以绑定到所有cpu。这样就均衡了。

但是，我执行过绑定之后，发现还是有一个cpu特别忙，观察发现，是一个中断特别忙，可以对应的找到接口是ens42，

好奇这个接口上面的ip我并没有用来指定发送，怎么会有中断呢？有中断代表有数据到达，通过tcpdump抓此接口上面的流量，发现的却是有的（注意关闭混杂模式 -p）,

于是想到是在局域网通信，那么可能是通信的时候对方将目标mac都写成这个接口的mac了，而mac得获得是通过
请求来得到的，在server端执行arping，发现此client上所有端口的ip的arp响应都是同一个mac，正是刚刚流量大的mac。

这说明arp应答有问题，在网上进行搜索，发现要改参数：
#+BEGIN_EXAMPLE
arp_announce = 2
arp_ignore = 1 
rp_filter = 1
#+END_EXAMPLE

有些地方说只改前面的就行，可是我如果改了前面的没改后面的导致arp响应完全没有了。

经过调整这几个参数后，再发送arp请求，发现应答正常了。

这几个参数留待分析。
*** 服务端参数
服务器端需要调整的参数与客户端有一些不同，除了文件数，还需要调整如下：
- /proc/sys/net/netfilter/nf_conntrack_max :: 设置最大跟踪的入链接数，大于此数会丢弃新链接
- /proc/sys/net/netfilter/nf_conntrack_tcp_timeout_established :: 设置合理的跟踪超时时间，默认5天太长，耗内存，改为30min








